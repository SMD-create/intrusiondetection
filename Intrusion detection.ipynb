{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f93d16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1         2   3    4     5   6   7   8   9   ...    33    34    35  \\\n",
      "0   0  tcp  ftp_data  SF  491     0   0   0   0   0  ...  0.17  0.03  0.17   \n",
      "1   0  udp     other  SF  146     0   0   0   0   0  ...  0.00  0.60  0.88   \n",
      "2   0  tcp   private  S0    0     0   0   0   0   0  ...  0.10  0.05  0.00   \n",
      "3   0  tcp      http  SF  232  8153   0   0   0   0  ...  1.00  0.00  0.03   \n",
      "4   0  tcp      http  SF  199   420   0   0   0   0  ...  1.00  0.00  0.00   \n",
      "\n",
      "     36    37    38    39    40       41  42  \n",
      "0  0.00  0.00  0.00  0.05  0.00   normal  20  \n",
      "1  0.00  0.00  0.00  0.00  0.00   normal  15  \n",
      "2  0.00  1.00  1.00  0.00  0.00  neptune  19  \n",
      "3  0.04  0.03  0.01  0.00  0.01   normal  21  \n",
      "4  0.00  0.00  0.00  0.00  0.00   normal  21  \n",
      "\n",
      "[5 rows x 43 columns]\n",
      "   0     1         2     3      4   5   6   7   8   9   ...    33    34    35  \\\n",
      "0   0   tcp   private   REJ      0   0   0   0   0   0  ...  0.04  0.06  0.00   \n",
      "1   0   tcp   private   REJ      0   0   0   0   0   0  ...  0.00  0.06  0.00   \n",
      "2   2   tcp  ftp_data    SF  12983   0   0   0   0   0  ...  0.61  0.04  0.61   \n",
      "3   0  icmp     eco_i    SF     20   0   0   0   0   0  ...  1.00  0.00  1.00   \n",
      "4   1   tcp    telnet  RSTO      0  15   0   0   0   0  ...  0.31  0.17  0.03   \n",
      "\n",
      "     36   37   38    39    40       41  42  \n",
      "0  0.00  0.0  0.0  1.00  1.00  neptune  21  \n",
      "1  0.00  0.0  0.0  1.00  1.00  neptune  21  \n",
      "2  0.02  0.0  0.0  0.00  0.00   normal  21  \n",
      "3  0.28  0.0  0.0  0.00  0.00    saint  15  \n",
      "4  0.02  0.0  0.0  0.83  0.71    mscan  11  \n",
      "\n",
      "[5 rows x 43 columns]\n",
      "WARNING:tensorflow:From D:\\SOFWARES\\Studies softwares\\Anaconda\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\SOFWARES\\Studies softwares\\Anaconda\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From D:\\SOFWARES\\Studies softwares\\Anaconda\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "493/493 [==============================] - 4s 5ms/step - loss: 0.2982 - val_loss: 0.8868\n",
      "Epoch 2/50\n",
      "493/493 [==============================] - 2s 4ms/step - loss: 0.2292 - val_loss: 0.8731\n",
      "Epoch 3/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2238 - val_loss: 0.8684\n",
      "Epoch 4/50\n",
      "493/493 [==============================] - 2s 4ms/step - loss: 0.2212 - val_loss: 0.8646\n",
      "Epoch 5/50\n",
      "493/493 [==============================] - 2s 4ms/step - loss: 0.2194 - val_loss: 0.8621\n",
      "Epoch 6/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2183 - val_loss: 0.8605\n",
      "Epoch 7/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2176 - val_loss: 0.8590\n",
      "Epoch 8/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2171 - val_loss: 0.8582\n",
      "Epoch 9/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2167 - val_loss: 0.8574\n",
      "Epoch 10/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2164 - val_loss: 0.8569\n",
      "Epoch 11/50\n",
      "493/493 [==============================] - 2s 4ms/step - loss: 0.2161 - val_loss: 0.8564\n",
      "Epoch 12/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2158 - val_loss: 0.8559\n",
      "Epoch 13/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2156 - val_loss: 0.8555\n",
      "Epoch 14/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2154 - val_loss: 0.8552\n",
      "Epoch 15/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2153 - val_loss: 0.8550\n",
      "Epoch 16/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2152 - val_loss: 0.8550\n",
      "Epoch 17/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2150 - val_loss: 0.8550\n",
      "Epoch 18/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2149 - val_loss: 0.8547\n",
      "Epoch 19/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2148 - val_loss: 0.8542\n",
      "Epoch 20/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2146 - val_loss: 0.8540\n",
      "Epoch 21/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2145 - val_loss: 0.8538\n",
      "Epoch 22/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2144 - val_loss: 0.8536\n",
      "Epoch 23/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2143 - val_loss: 0.8536\n",
      "Epoch 24/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2142 - val_loss: 0.8535\n",
      "Epoch 25/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2142 - val_loss: 0.8534\n",
      "Epoch 26/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2141 - val_loss: 0.8534\n",
      "Epoch 27/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2140 - val_loss: 0.8533\n",
      "Epoch 28/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2139 - val_loss: 0.8532\n",
      "Epoch 29/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2139 - val_loss: 0.8529\n",
      "Epoch 30/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2138 - val_loss: 0.8528\n",
      "Epoch 31/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2138 - val_loss: 0.8527\n",
      "Epoch 32/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2137 - val_loss: 0.8527\n",
      "Epoch 33/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2137 - val_loss: 0.8526\n",
      "Epoch 34/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2137 - val_loss: 0.8526\n",
      "Epoch 35/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2137 - val_loss: 0.8526\n",
      "Epoch 36/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2137 - val_loss: 0.8527\n",
      "Epoch 37/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2137 - val_loss: 0.8525\n",
      "Epoch 38/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2136 - val_loss: 0.8526\n",
      "Epoch 39/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2136 - val_loss: 0.8525\n",
      "Epoch 40/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2136 - val_loss: 0.8525\n",
      "Epoch 41/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2136 - val_loss: 0.8525\n",
      "Epoch 42/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2136 - val_loss: 0.8525\n",
      "Epoch 43/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2136 - val_loss: 0.8525\n",
      "Epoch 44/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2136 - val_loss: 0.8525\n",
      "Epoch 45/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2136 - val_loss: 0.8525\n",
      "Epoch 46/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2136 - val_loss: 0.8524\n",
      "Epoch 47/50\n",
      "493/493 [==============================] - 2s 4ms/step - loss: 0.2136 - val_loss: 0.8524\n",
      "Epoch 48/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2136 - val_loss: 0.8524\n",
      "Epoch 49/50\n",
      "493/493 [==============================] - 2s 3ms/step - loss: 0.2136 - val_loss: 0.8524\n",
      "Epoch 50/50\n",
      "493/493 [==============================] - 2s 4ms/step - loss: 0.2135 - val_loss: 0.8524\n",
      "705/705 [==============================] - 2s 2ms/step\n",
      "Number of anomalies detected: 1128\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "import numpy as np\n",
    "\n",
    "# Load the NSL-KDD training and testing datasets\n",
    "train_data = pd.read_csv('KDDTrain+.txt', header=None)\n",
    "test_data = pd.read_csv('KDDTest+.txt', header=None)\n",
    "\n",
    "# Print the first few rows of the datasets to inspect the structure\n",
    "print(train_data.head())\n",
    "print(test_data.head())\n",
    "\n",
    "# Define column names for the NSL-KDD dataset\n",
    "columns = [\n",
    "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
    "    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
    "    'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
    "    'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
    "    'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',\n",
    "    'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate',\n",
    "    'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
    "    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
    "    'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'label'\n",
    "]\n",
    "\n",
    "# Adjust if there is an extra column by removing the last column or verifying splitting\n",
    "if train_data.shape[1] == 43 and test_data.shape[1] == 43:\n",
    "    train_data.drop(columns=[train_data.columns[-1]], inplace=True)\n",
    "    test_data.drop(columns=[test_data.columns[-1]], inplace=True)\n",
    "    train_data.columns = columns\n",
    "    test_data.columns = columns\n",
    "else:\n",
    "    print(\"Column length mismatch persists. Further inspection needed.\")\n",
    "\n",
    "# Proceed only if column names are correctly assigned\n",
    "if 'label' in train_data.columns and 'label' in test_data.columns:\n",
    "    # Separate features and labels\n",
    "    X_train = train_data.drop(columns=['label'])\n",
    "    X_test = test_data.drop(columns=['label'])\n",
    "\n",
    "    # Preprocess categorical features\n",
    "    categorical_features = ['protocol_type', 'service', 'flag']\n",
    "    numeric_features = X_train.columns.difference(categorical_features)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "            ('cat', OneHotEncoder(), categorical_features)\n",
    "        ])\n",
    "\n",
    "    # Create a preprocessing and training pipeline\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "    # Fit and transform the training data\n",
    "    X_train_scaled = pipeline.fit_transform(X_train)\n",
    "    X_test_scaled = pipeline.transform(X_test)\n",
    "\n",
    "    # Define the autoencoder model\n",
    "    input_dim = X_train_scaled.shape[1]\n",
    "    encoding_dim = 14  # Dimension of the latent space\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "    decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
    "\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    history = autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                              epochs=50,\n",
    "                              batch_size=256,\n",
    "                              shuffle=True,\n",
    "                              validation_data=(X_test_scaled, X_test_scaled))\n",
    "\n",
    "    # Predict on the test set\n",
    "    X_test_pred = autoencoder.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate the reconstruction error\n",
    "    mse = np.mean(np.power(X_test_scaled - X_test_pred, 2), axis=1)\n",
    "\n",
    "    # Set a threshold for anomaly detection (95th percentile)\n",
    "    threshold = np.percentile(mse, 95)\n",
    "\n",
    "    # Flag anomalies\n",
    "    anomalies = mse > threshold\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Number of anomalies detected: {np.sum(anomalies)}\")\n",
    "else:\n",
    "    print(\"Column assignment failed. Please check the dataset and column names.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9640c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# Define the autoencoder model\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "encoding_dim = 14  # Dimension of the latent space\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoder = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = autoencoder.fit(X_train_scaled, X_train_scaled,\n",
    "                          epochs=50,\n",
    "                          batch_size=256,\n",
    "                          shuffle=True,\n",
    "                          validation_data=(X_test_scaled, X_test_scaled))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
